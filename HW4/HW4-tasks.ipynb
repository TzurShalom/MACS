{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuCFdN40kD3N"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Assignment 4: Differentiation and optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQdRNfPkD3N"
      },
      "source": [
        "import sympy as sym # symbolic differentiation\n",
        "import jax          # algorithmic differentiation\n",
        "import jax.numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NFDA3uFHcf_"
      },
      "source": [
        "## Question 1: Differentiations\n",
        "\n",
        "Function\n",
        "\n",
        "$$f(a, b) = \\frac a b cos(a)^2 \\exp \\left( - \\frac {a^2} {b^2}\\right)$$\n",
        "\n",
        "is given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvlvWUY1WdMZ"
      },
      "source": [
        "1. Derive the partial derivatives of $f(a, b)$ by $a$ and $b$. (Write them in this text)\n",
        "\n",
        "    $$\\frac {\\partial f} {\\partial a} = $$\n",
        "    $$\\frac {\\partial f} {\\partial b} = $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teHG_JS4ZE9s"
      },
      "source": [
        "2. Implement the derivatives as Python functions:\n",
        "  1. According to symbolic expressions. (print the expressions)\n",
        "  2. Using algorithmic differentiation (with `jax`).\n",
        "      * For a two variable function, $ f(a,b) $, its gradients by $x$ and $y$ can be obtained like this: df_dxy = jax.grad(f, argnums=(0, 1)). The x, and y derivatives are then: df_dxy[0] = df_dx, and df_dxy[1] = df_dy.\n",
        "\n",
        "$-$\n",
        "\n",
        "  \n",
        "Compare the accuracy of symbolic and algorithmic differentiation:\n",
        "\n",
        "\n",
        "1.   Choose a number of points in the range: $a\\in(-20, 20)$, and $b\\in(1, 100)$.\n",
        "2.   Calculate the squared error between simbolic and algorithmic differentiation for each set of points $(a,b)$.\n",
        "3. Average all the errors. (print the average error obtained)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJV-ytPHZJRR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K95Vz9uTWzv5"
      },
      "source": [
        "3. Plot\n",
        "  * $f(a, 10)$, $\\frac {\\partial f(a, 10)} {\\partial a}$ for range $a \\in [-20, 20]$,\n",
        "  * $f(10, b)$, $\\frac {\\partial f(10, b)} {\\partial b}$ for range $b \\in 1, 100$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKErhdqUYhw6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT8qvZtLZVpr"
      },
      "source": [
        "4. Implement a function for approximate numerical differentiation, given the difference size $h$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiylvd_FZnjB"
      },
      "source": [
        "def diff(f, x, h):\n",
        "  \"\"\"Differentiates f at x numerically, using h as the difference.\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvamVGzCZzeT"
      },
      "source": [
        "5. Find the best difference size $h$ for differentiating\n",
        "   * $f(3, 10)$ by $a$.\n",
        "   * $f(1, 1)$ by $b$.\n",
        "\n",
        "(Print the best step size, the derivative obtained and the difference between the numerical and the exact (algorithmic or symbolic) derivatives)\n",
        "\n",
        "The best difference size minimizes the error of numerical differentiation relative to the exact differentiation.\n",
        "\n",
        "Note: In this excercise it is recommended to use gradient descent, however a simple search is also accepted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-_-Mpn1aHuL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKLhu3Z9a3D5"
      },
      "source": [
        "## Question 2: Optimization\n",
        "\n",
        "### Logistic regression\n",
        "\n",
        "Logistic regression is a statiscal model that models the probability of an event happening. In binary logistic regression there is a single binary dependent variable coded by '0' or '1'.\n",
        "\n",
        "We can express the data as a tuple $(x,y)=(data,outcome)=(x,0/1)$\n",
        "\n",
        "To model the probability of event $x$, we can use the logistic function $p(x)=\\frac{1}{1+e^{-(x-\\mu)/s}}$.\n",
        "\n",
        "To find the appropiate $\\mu$, that maximizes the classifaction accuracy we minimize the loss function $-\\sum_{i=1}^N (y_i \\log p(x_i) + (1 - y_i) \\log (1 - p(x_i)))$.\n",
        "\n",
        "In some cases, we give more importance to correctly classify one event over an other (like contracting an illness over not contracting it), and one way to express this is to add a \"fixing parameter\" in the loss function. One way this can be achieved is by adding weights: $-\\sum_{i=1}^N (W_1y_i \\log p(x_i) + W_2(1 - y_i) \\log (1 - p(x_i)))$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a trial group of 20 rats, ratio of time exposed to a virus and the event of contracting that virus are given as a list of pairs (ratio, illness) (1 corresponds to having the illness):"
      ],
      "metadata": {
        "id": "eExKcFRWmh-T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VHUHiJSbDZg"
      },
      "source": [
        "rats = [(0.47, 0), (0.23, 0), (0.86, 1), (0.22, 0), (0.21, 1),\n",
        "        (0.31, 0), (0.62, 0), (0.941, 1), (0.27, 0), (0.35, 1),\n",
        "        (0.18, 0), (0.13, 0), (0.31, 1), (0.99, 1), (0.85, 1),\n",
        "        (0.35, 1), (0.6, 1), (0.89, 0), (0.6, 1), (0.92, 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNnmQ8d2fAOl"
      },
      "source": [
        "We want to predict rat illness based on the ratio of exposure to a virus. The prediction function is\n",
        "\n",
        "$$illness = ratio \\ge threshold.$$\n",
        "\n",
        "The loss for this _classification_ problem is:\n",
        "\n",
        "\\begin{aligned}\n",
        "& L = -\\sum_{i=1}^N (W_1illness_i \\log p_i + W_2(1 - illness_i) \\log (1 - p_i)) \\\\\n",
        "\\mbox{where} & \\\\\n",
        "& p_i = \\frac 1 {1 + \\exp(threshold - ratio_i)}\n",
        "\\\\~\\\\\n",
        "\\mbox{and } &W_{1}, W_{2} \\mbox{ are weights}\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky3bsw8EgZsJ"
      },
      "source": [
        "1. Implement the loss as a function of the threshold, and weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZjJjUUWghq-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j22eKFdKgkti"
      },
      "source": [
        "2. Plot the loss and the derivative of the loss by the threshold in the range $threshold \\in (0.01, 0.99)$, and $ W_1, W_2 = 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFAjQG9Ug052"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxEbOEdhBKf"
      },
      "source": [
        "3. Find the best threshold using gradient descent, and $ W_1, W_2$ such that the number of true positives (rats which have the illness and were correctly classified) is greater than $80\\%$, but the number of false positives ( rats which dont have the illness and were misclassified) is not $100\\%$. (Note: you can fix $W_2$ to be 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0MMAREahDrT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvD-L4rnhGCN"
      },
      "source": [
        "4. Find the best threshold using Newton's method, given $W_1, W_2$ from previous question. (Newton's method was explained in class). How many iterations were needed?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkF9ML7ohIR4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J17VCw3jlFWm"
      },
      "source": [
        "5. Show actual vs. predicted recoveries as a scatter plot for the best threshold. How many recoveries were misclassified?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGs13-yzlS3B"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}